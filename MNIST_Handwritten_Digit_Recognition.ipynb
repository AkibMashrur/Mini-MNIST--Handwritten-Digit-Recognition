{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Handwritten Digit Recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0F3cZ2sfWvTM",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ec84f57d-a55a-4a15-d22a-60c0d283c3f8"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5fc0a968-669f-47eb-b684-fcbbc72aeae8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5fc0a968-669f-47eb-b684-fcbbc72aeae8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving reduced_mnist.csv to reduced_mnist.csv\n",
            "User uploaded file \"reduced_mnist.csv\" with length 4610614 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vl9kL6VFHhs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A small subset of MNIST dataset for handwritten digits has been used in this part for multiclass-classification. \n",
        "\n",
        "In the first cell, all relevant python modules have been imported."
      ]
    },
    {
      "metadata": {
        "id": "ajpqH9FdX5J5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Data Preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "\n",
        "## Sklearn Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Performance Measures\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "## Visualisation\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BI2TNz01Jyyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2.1 Read and understand the data, create a default One-vs-Rest Classifier**\n",
        "\n",
        "*2.1(1) Use the data from the file reduced_mnist.csv in the data directory. Begin by reading the data. Print the following*\n",
        "information:\n",
        "\n",
        "*   Number of data points\n",
        "*   Total number of features\n",
        "*   Unique labels in the data*"
      ]
    },
    {
      "metadata": {
        "id": "6CtjR-MrX54P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f71699b6-0163-4f7e-87f0-648a66419ee4"
      },
      "cell_type": "code",
      "source": [
        "##### 2.1 (1) #####\n",
        "\n",
        "mnist = pd.read_csv(\"reduced_mnist.csv\")\n",
        "unique_labels = mnist['label'].nunique()\n",
        "# We can see from the dataset all the feature columns starts with \"pixel\"\n",
        "mnist_features = mnist.filter(regex= '^pixel', axis=1)\n",
        "mnist_number_of_examples, mnist_number_of_features = mnist_features.shape\n",
        "datapoints = mnist_number_of_examples * mnist_number_of_features\n",
        "print(f\"The number of examples in the dataset are: {mnist_number_of_examples}\")\n",
        "print(f\"The number of features in the dataset are: {mnist_number_of_features}\")\n",
        "print(f\"The number of datapoints are {datapoints}\")\n",
        "print(f\"\\nThe number of unique labels are: {unique_labels}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of examples in the dataset are: 2520\n",
            "The number of features in the dataset are: 784\n",
            "The number of datapoints are 1975680\n",
            "\n",
            "The number of unique labels are: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MhM7FoZoMbhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Explanation:** \n",
        "\n",
        "We can see that the number of unique labels are more than 2, therefore it is a multiclass classification problem."
      ]
    },
    {
      "metadata": {
        "id": "aRXSFHn9Lxqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*2.1(2) Split the data into 70% training data and 30% test data. \n",
        "Fit a One-vs-Rest Classifier (which uses Logistic regression classifier with alpha=1) on training data, and report accuracy, precision, recall on testing data.*"
      ]
    },
    {
      "metadata": {
        "id": "kaBUlw-xYGVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "2b386ca9-944a-4551-cca4-c06b1cdde3e7"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the dataset i\n",
        "\n",
        "mnist_train, mnist_test = train_test_split(mnist, test_size=0.3, \n",
        "                                           random_state = 2018)\n",
        "\n",
        "# Splitting train and test\n",
        "\n",
        "mnist_train_X = mnist_train.iloc[:,1:]\n",
        "mnist_train_y = mnist_train.iloc[:,:1]\n",
        "\n",
        "mnist_test_X = mnist_test.iloc[:,1:]\n",
        "mnist_test_y = mnist_test.iloc[:,:1]\n",
        "\n",
        "# fitting OnevsRest Classifier\n",
        "\n",
        "alpha = 1\n",
        "ovr = OneVsRestClassifier(LogisticRegression(penalty = 'l1',C = 1/alpha, random_state = 2018))\n",
        "ovr.fit(mnist_train_X,mnist_train_y)\n",
        "\n",
        "# Making predictions\n",
        "\n",
        "y_pred_ovr = ovr.predict(mnist_test_X)\n",
        "\n",
        "ovr_accuracy = accuracy_score(mnist_test_y, y_pred_ovr)\n",
        "ovr_precision = precision_score(mnist_test_y, y_pred_ovr,average = 'weighted')\n",
        "ovr_recall = recall_score(mnist_test_y, y_pred_ovr,average= 'weighted')\n",
        "\n",
        "print(f\"average accuracy of the model is {ovr_accuracy}\")\n",
        "print(f\"average precision of the model is {ovr_precision}\")\n",
        "print(f\"average recall of the model is {ovr_recall}\")\n",
        "\n",
        "#Performance Measures\n",
        "\n",
        "precision_1, recall_1, fscore_1, support_1 = score(mnist_test_y.values.ravel(),\n",
        "                                           y_pred_ovr)\n",
        "\n",
        "#Confusion Metrices\n",
        "\n",
        "model_cm_ovr_l1_1 = confusion_matrix(mnist_test_y.values.ravel(),\n",
        "                                           y_pred_ovr)\n",
        "\n",
        "\n",
        "print(f\"Confusion Matrix for model is: \\n {model_cm_ovr_l1_1} \\n\")\n",
        "\n",
        "accuracy_per_class = np.diag(model_cm_ovr_l1_1)/np.sum(model_cm_ovr_l1_1, axis = 1)\n",
        "\n",
        "Performance_matrix_per_class_1 = pd.DataFrame()\n",
        "Performance_matrix_per_class_1['class'] = np.array(list(range(0,10)))\n",
        "Performance_matrix_per_class_1['accuracy'] = pd.Series(accuracy_per_class)\n",
        "Performance_matrix_per_class_1['precision'] = pd.Series(precision_1)\n",
        "Performance_matrix_per_class_1['recall'] = pd.Series(recall_1)\n",
        "Performance_matrix_per_class_1['fscore'] = pd.Series(fscore_1)\n",
        "Performance_matrix_per_class_1['support'] = pd.Series(support_1)\n",
        "\n",
        "Performance_matrix_per_class_1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average accuracy of the model is 0.8253968253968254\n",
            "average precision of the model is 0.8278211751410696\n",
            "average recall of the model is 0.8253968253968254\n",
            "Confusion Matrix for model is: \n",
            " [[74  0  1  1  0  0  1  0  7  0]\n",
            " [ 0 94  0  0  0  0  0  0  0  0]\n",
            " [ 0  2 52  3  2  1  5  1  3  4]\n",
            " [ 1  0  2 61  0  5  1  2  3  0]\n",
            " [ 3  1  0  1 60  1  1  1  1  9]\n",
            " [ 1  2  0  1  1 47  0  1 10  1]\n",
            " [ 3  0  0  0  0  2 64  0  1  0]\n",
            " [ 2  0  1  2  1  0  0 66  1  2]\n",
            " [ 2  6  2  4  0  3  2  1 53  4]\n",
            " [ 1  0  0  3  3  1  0  4  1 53]] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fscore</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.850575</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.865497</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.944724</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.712329</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.712329</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.802632</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.807947</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.783333</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.758065</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.874172</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.688312</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.688312</td>\n",
              "      <td>0.675159</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.803030</td>\n",
              "      <td>0.726027</td>\n",
              "      <td>0.803030</td>\n",
              "      <td>0.762590</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  accuracy  precision    recall    fscore  support\n",
              "0      0  0.880952   0.850575  0.880952  0.865497       84\n",
              "1      1  1.000000   0.895238  1.000000  0.944724       94\n",
              "2      2  0.712329   0.896552  0.712329  0.793893       73\n",
              "3      3  0.813333   0.802632  0.813333  0.807947       75\n",
              "4      4  0.769231   0.895522  0.769231  0.827586       78\n",
              "5      5  0.734375   0.783333  0.734375  0.758065       64\n",
              "6      6  0.914286   0.864865  0.914286  0.888889       70\n",
              "7      7  0.880000   0.868421  0.880000  0.874172       75\n",
              "8      8  0.688312   0.662500  0.688312  0.675159       77\n",
              "9      9  0.803030   0.726027  0.803030  0.762590       66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "dkmLyR_0So9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Explanation**: \n",
        "\n",
        "We can see from the above performance table that the model is being able to predict 1's and 6's accurately, whereas, it not being able to predict 8's and 9's as accurately. Therefore these classes have relatively low performance measures."
      ]
    },
    {
      "metadata": {
        "id": "RcDXp9LSTqWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2.2 Choosing the best hyper-parameter**\n",
        "\n",
        "\n",
        "2.2(1) For this section I have created 10 random splits of training data into training and validation data. The\n",
        "best value of alpha from the following set: {0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333} has been chosen. To choose the best\n",
        "alpha hyperparameter value, I have done the following :\n",
        "\n",
        "\n",
        "*   For each value of hyperparameter, perform 10 random splits of training data into training and validation data\n",
        "as said above.\n",
        "*   For each value of hyperparameter, use its 10 random splits and find the average training and validation\n",
        "accuracy.\n",
        "*   On a graph, plot both the average training accuracy (in red) and average validation accuracy (in blue) w.r.t.\n",
        "each hyperparameter setting. Comment on this graph by identifying regions of overfitting and underfitting.\n",
        "*   Print the best value of alpha hyperparameter.\n"
      ]
    },
    {
      "metadata": {
        "id": "Exnn6qoSYKIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "4c415024-ddc8-4599-b9ef-f15b08ac93b8"
      },
      "cell_type": "code",
      "source": [
        "## Building the function to calculate training accuracy and validation accuracy for each iteration\n",
        "\n",
        "def runOVRmodel(trials, train_data, penalty_type, penalty_score):\n",
        "\n",
        "   val_acc = 0\n",
        "   train_acc = 0\n",
        "   \n",
        "   for i in range(0,trials):\n",
        "      Dtrain, Dtest = train_test_split(train_data, test_size=0.3)\n",
        "      ovr = OneVsRestClassifier(LogisticRegression(C=1/penalty_score, \n",
        "                                                   penalty=penalty_type))\n",
        "      ovr.fit(Dtrain.iloc[:,1:785], Dtrain.iloc[:,0:1])\n",
        "      y_predict_ovr = ovr.predict(mnist_test_X)\n",
        "      train_acc += accuracy_score(ovr.predict(mnist_train_X), mnist_train_y)\n",
        "      val_acc += accuracy_score(y_predict_ovr, mnist_test_y)\n",
        "      #model_weights += np.append(ovr.intercept_, ovr.coef_)\n",
        "\n",
        "   val_acc /= trials\n",
        "   train_acc /= trials\n",
        "   #model_weights /= trials\n",
        "\n",
        "   return np.round(val_acc, decimals=2), np.round(train_acc, decimals=2) \n",
        "  \n",
        "# Instantiating the required lists\n",
        "\n",
        "alpha_vals = [.1,1,3,10,33,100,333,1000,3333,10000,33333]\n",
        "val_acc = np.zeros(len(alpha_vals))\n",
        "train_acc = np.zeros(len(alpha_vals))\n",
        "index = 0\n",
        "# Iterating the model\n",
        "for l in alpha_vals:\n",
        "   val_acc[index], train_acc[index] = runOVRmodel(10,mnist_train, 'l1', \n",
        "                                                  np.float(l))\n",
        "   index += 1\n",
        "\n",
        "print(\"Training Accuracy: {}\".format(train_acc))\n",
        "print(\"Validation Accuracy: {}\".format(val_acc))\n",
        "\n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_ovr  = np.argmax(val_acc)\n",
        "best_alpha = alpha_vals[max_index_ovr]\n",
        "print(\"Best Alpha: {}\".format(best_alpha))\n",
        "\n",
        "#plot the accuracy curve\n",
        "plt.plot(range(0,len(alpha_vals)), val_acc, color='b', \n",
        "         label='Validation Accuracy')\n",
        "plt.plot(range(0,len(alpha_vals)), train_acc, color='r', \n",
        "         label='Training Accuracy')\n",
        "#replace the x-axis labels with penalty values\n",
        "plt.xticks(range(0,len(alpha_vals)), alpha_vals, rotation='vertical')\n",
        "\n",
        "#Highlight the best values of alpha\n",
        "plt.plot((max_index_ovr, max_index_ovr), (0, val_acc[max_index_ovr]), \n",
        "         ls='dotted', color='b')\n",
        "\n",
        "\n",
        "#Set the y-axis from 0 to 1.0\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 1.0])\n",
        "\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: [0.95 0.95 0.95 0.95 0.95 0.96 0.94 0.9  0.84 0.73 0.56]\n",
            "Validation Accuracy: [0.82 0.82 0.83 0.82 0.84 0.84 0.86 0.84 0.79 0.7  0.53]\n",
            "Best Alpha: 333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEOCAYAAACUxJyzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f098347da20>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0n54ByeIWNvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "We can see from the above graph that we can find the best fitting model at Alpha of 333.\n",
        "\n",
        "**Region of Overfit:**\n",
        "we get overfit models for alpha value ranging between 0.1 and 100, because we have very high training accuracy but comparatively low validation accuracy for these values.\n",
        "\n",
        "\n",
        "**Region of Underfit:**\n",
        "we get overfit models for alpha value ranging between 3333 and 33333, because we have low training and validation accuracy for these values."
      ]
    },
    {
      "metadata": {
        "id": "W48qE7WeXxxi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2.2(2)- For this section, I have evaluated the prediction performance on test data and reported the following:\n",
        "\n",
        "\n",
        "*   Total number of non-zero features in the final model.\n",
        "*   The confusion matrix\n",
        "*   Precision, recall and accuracy for each class.\n",
        "\n",
        "\n",
        "Finally, I have discussed if there is any sign of underfitting or overfitting with appropriate reasoning"
      ]
    },
    {
      "metadata": {
        "id": "oZpvW9mwYU7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "4dbb5d11-56e8-4d16-8b84-dcac0e8fff29"
      },
      "cell_type": "code",
      "source": [
        "#Building a new linear regression model based on best alpha\n",
        "\n",
        "ovr_alpha = OneVsRestClassifier(LogisticRegression(C= 1/best_alpha, \n",
        "                                                   penalty='l1'))\n",
        "ovr_alpha_model = ovr_alpha.fit(mnist_train_X,mnist_train_y)\n",
        "\n",
        "y_pred_ovr_best = ovr_alpha_model.predict(mnist_test_X)\n",
        "\n",
        "#Creating a weight matrix\n",
        "\n",
        "feature_weights_ovr=ovr_alpha_model.coef_[0]\n",
        "Weight_matrix_ovr = pd.DataFrame()\n",
        "Weight_matrix_ovr['Feature'] = pd.Series(list(mnist_features.columns.values))\n",
        "Weight_matrix_ovr['Weights'] = pd.Series(feature_weights_ovr,name= \"Weights\")\n",
        "Weight_matrix_ovr['Abs Weights'] = abs(Weight_matrix_ovr['Weights'])\n",
        "\n",
        "#Masking non-zero features in the weight matrix\n",
        "mask = Weight_matrix_ovr[Weight_matrix_ovr.drop('Feature', axis=1) != 0]\n",
        "shape_of_matrix = mask.loc[mask.dropna(thresh=1).index].shape\n",
        "\n",
        "#Identifying the number of non-zero features \n",
        "print(f\"The number of non-zero features in the matrix is:{shape_of_matrix[0]}\")\n",
        "\n",
        "#Confusion Metrices\n",
        "\n",
        "model_cm_ovr_l1 = confusion_matrix(y_pred_ovr_best, \n",
        "                                   mnist_test_y.values.ravel())\n",
        "print(f\"Confusion Matrix for model with best alpha is: \\n {model_cm_ovr_l1} \\n\")\n",
        "\n",
        "#Performance Measures\n",
        "      \n",
        "accuracy = np.diag(model_cm_ovr_l1)/np.sum(model_cm_ovr_l1, axis = 1)\n",
        "\n",
        "precision, recall, fscore, support = score(mnist_test_y.values.ravel(),\n",
        "                                           y_pred_ovr_best)\n",
        "\n",
        "Performance_matrix_per_class = pd.DataFrame()\n",
        "Performance_matrix_per_class['class'] = np.array(list(range(0,10)))\n",
        "Performance_matrix_per_class['accuracy'] = pd.Series(accuracy)\n",
        "Performance_matrix_per_class['precision'] = pd.Series(precision)\n",
        "Performance_matrix_per_class['recall'] = pd.Series(recall)\n",
        "Performance_matrix_per_class['fscore'] = pd.Series(fscore)\n",
        "Performance_matrix_per_class['support'] = pd.Series(support)\n",
        "\n",
        "print(Performance_matrix_per_class)\n",
        "\n",
        "Performance_comparison = pd.DataFrame()\n",
        "Performance_comparison['class'] = np.array(list(range(0,10)))\n",
        "Performance_comparison['validation accuracy'] = pd.Series(accuracy_per_class)\n",
        "Performance_comparison['testing accuracy'] = pd.Series(accuracy)\n",
        "\n",
        "print(Performance_comparison)\n",
        "print(f\"Average validation accuracy is {np.average(accuracy_per_class)}\")\n",
        "print(f\"Average testing accuracy is {np.average(accuracy)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of non-zero features in the matrix is:86\n",
            "Confusion Matrix for model with best alpha is: \n",
            " [[79  0  0  0  1  1  1  0  1  1]\n",
            " [ 0 94  3  1  1  2  0  0  3  0]\n",
            " [ 1  0 55  2  0  0  0  2  1  0]\n",
            " [ 1  0  2 61  1  4  0  0  2  3]\n",
            " [ 0  0  1  0 69  1  0  1  1  1]\n",
            " [ 0  0  0  7  1 47  2  0  1  2]\n",
            " [ 0  0  6  1  2  1 66  0  1  0]\n",
            " [ 0  0  1  1  1  0  0 69  1  4]\n",
            " [ 3  0  3  1  0  8  1  0 63  1]\n",
            " [ 0  0  2  1  2  0  0  3  3 54]] \n",
            "\n",
            "   class  accuracy  precision    recall    fscore  support\n",
            "0      0  0.940476   0.940476  0.940476  0.940476       84\n",
            "1      1  0.903846   0.903846  1.000000  0.949495       94\n",
            "2      2  0.901639   0.901639  0.753425  0.820896       73\n",
            "3      3  0.824324   0.824324  0.813333  0.818792       75\n",
            "4      4  0.932432   0.932432  0.884615  0.907895       78\n",
            "5      5  0.783333   0.783333  0.734375  0.758065       64\n",
            "6      6  0.857143   0.857143  0.942857  0.897959       70\n",
            "7      7  0.896104   0.896104  0.920000  0.907895       75\n",
            "8      8  0.787500   0.787500  0.818182  0.802548       77\n",
            "9      9  0.830769   0.830769  0.818182  0.824427       66\n",
            "   class  validation accuracy  testing accuracy\n",
            "0      0             0.880952          0.940476\n",
            "1      1             1.000000          0.903846\n",
            "2      2             0.712329          0.901639\n",
            "3      3             0.813333          0.824324\n",
            "4      4             0.769231          0.932432\n",
            "5      5             0.734375          0.783333\n",
            "6      6             0.914286          0.857143\n",
            "7      7             0.880000          0.896104\n",
            "8      8             0.688312          0.787500\n",
            "9      9             0.803030          0.830769\n",
            "Average validation accuracy is 0.8195847956267477\n",
            "Average testing accuracy is 0.8657567762690714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Fq7iS3gfzhO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "Based on the above result we can see that there is no such sign of overfitting or underfitting since we are getting reatively high average testing accuracy of the model compared to the validation scores. However, testing accuracy is relatively low for digit 5 and digit 8, indicating that model is not being able to accurately regonize these two digits.  "
      ]
    },
    {
      "metadata": {
        "id": "9cTo2tCRdGls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}